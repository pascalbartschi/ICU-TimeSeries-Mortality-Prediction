{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2405308b-b436-407c-8e71-c70884fc43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from chronos import BaseChronosPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from modules import * \n",
    "\n",
    "# override all pandas display limits\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa398f-7502-45e1-b2fd-c25a577c80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-a-filled.parquet -> set_a_filled\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-b.parquet -> set_b\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-a.parquet -> set_a\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-c-filled.parquet -> set_c_filled\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-c.parquet -> set_c\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-b-filled.parquet -> set_b_filled\n"
     ]
    }
   ],
   "source": [
    "# load parquet files\n",
    "data_path = Path(\"../../data\")\n",
    "notebooks_path = Path(os.getcwd())\n",
    "data_dir = {}\n",
    "\n",
    "##unsafe\n",
    "# for file_path in list((notebooks_path / data_path).glob(\"*.parquet\")):\n",
    "#     print(f\"Reading {file_path}\")\n",
    "#     # retrieve the name of the file without the extension for all OS\n",
    "#     data = pd.read_parquet(file_path)\n",
    "#     # if \"Time\" in df.columns:\n",
    "#     #     df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "#     data_dir[str(file_path).replace(\"\\\\\", \"/\").split(\"/\")[-1].split(\".\")[0].replace(\"-\", \"_\")] = data\n",
    "\n",
    "for file_path in (notebooks_path / data_path).glob(\"set*.parquet\"):\n",
    "    var_name = file_path.stem.replace(\"-\", \"_\")\n",
    "    globals()[var_name] = pd.read_parquet(file_path)\n",
    "    print(f\"Reading {file_path} -> {var_name}\")\n",
    "\n",
    "\n",
    "ID_vars = [\"PatientID\", \"Time\", \"RecordID\"] # TODO \n",
    "# stationary variables\n",
    "stationary_vars = [\"Age\", \"Gender\", \"Height\"] #, \"ICUType\"]\n",
    "# dynamic variables\n",
    "dynamic_vars = set_a.columns.difference(stationary_vars + ID_vars + ['In-hospital_death']).tolist()\n",
    "\n",
    "feature_cols = dynamic_vars + stationary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5ce5cc-b1d4-43bd-bd0a-93c2124d4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parquet_for_lstm(key, scaler=None, fit_scaler=False):\n",
    "    labelname = 'In-hospital_death'\n",
    "    df = globals()[f\"set_{key}_filled\"].copy()\n",
    "\n",
    "    # Sort and fill NaNs\n",
    "    df = df.sort_values([\"RecordID\", \"Time\"])\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "    # raise NotImplementedError(\"Encode cathegories\")\n",
    "\n",
    "    # --- Fit scaler on all feature data if requested ---\n",
    "    if fit_scaler or scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[feature_cols])\n",
    "\n",
    "    # --- Apply scaling ---\n",
    "    df[feature_cols] = scaler.transform(df[feature_cols])\n",
    "\n",
    "    # Group by patient\n",
    "    X = []\n",
    "    y = []\n",
    "    for pid, group in df.groupby(\"RecordID\"):\n",
    "        group = group.sort_values(\"Time\")\n",
    "        X.append(group[feature_cols].values)\n",
    "        y.append(group[labelname].iloc[0])\n",
    "\n",
    "    X_tensor = torch.tensor(np.stack(X)).float()  # (n_patients, seq_len, n_features)\n",
    "    y_tensor = torch.tensor(y).float()            # (n_patients,)\n",
    "\n",
    "    return X_tensor, y_tensor, scaler  # return scaler for reuse on val/test\n",
    "\n",
    "\n",
    "X_train, y_train, fitted_scaler = preprocess_parquet_for_lstm(\"a\", fit_scaler = True)\n",
    "# len(preprocess_parquet_for_lstm(\"b\", scaler = fitted_scaler))\n",
    "X_val, y_val , _    = preprocess_parquet_for_lstm(\"b\", scaler = fitted_scaler)\n",
    "X_test, y_test , _  = preprocess_parquet_for_lstm(\"c\", scaler = fitted_scaler)\n",
    "\n",
    "\n",
    "# extract dimensions\n",
    "num_patients, sequence_length, num_features  = X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ce486f-cb67-4fca-a6fc-8ce42797b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load the pretrained Chronos model\n",
    "model_id = \"amazon/chronos-t5-small\"  # Example model; choose the one that fits your needs\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # use \"amazon/chronos-bolt-small\" for the corresponding Chronos-Bolt model\n",
    "    device_map=device,  # use \"cpu\" for CPU inference\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "# Manually move tokenizer boundaries to match model device\n",
    "pipeline.tokenizer.boundaries = pipeline.tokenizer.boundaries.to(pipeline.model.device)\n",
    "\n",
    "# Fix Chronos tokenizer device mismatch when appending EOS token\n",
    "def patched_append_eos_token(self, token_ids, attention_mask):\n",
    "    device = token_ids.device\n",
    "    batch_size = token_ids.shape[0]\n",
    "    eos_tokens = torch.full((batch_size, 1), fill_value=self.config.eos_token_id, device=device)\n",
    "    eos_mask = torch.full((batch_size, 1), fill_value=True, device=device)\n",
    "    token_ids = torch.concat((token_ids, eos_tokens), dim=1)\n",
    "    attention_mask = torch.concat((attention_mask, eos_mask), dim=1)\n",
    "    return token_ids, attention_mask\n",
    "\n",
    "# Patch the method\n",
    "pipeline.tokenizer._append_eos_token = patched_append_eos_token.__get__(pipeline.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c469730-ddbb-432e-9781-ecbdf082bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_embeddings = get_chronos_embeddings(X_train, num_features, pipeline, return_stack = False)\n",
    "# X_val_embeddings = get_chronos_embeddings(X_val, num_features, pipeline, return_stack = False)\n",
    "# X_test_embeddings = get_chronos_embeddings(X_test, num_features, pipeline, return_stack = False)\n",
    "# torch.save(X_train_embeddings, notebooks_path / data_path / Path('X_train_embeddings_Q431.pt'))\n",
    "# torch.save(X_val_embeddings, notebooks_path / data_path / Path('X_val_embeddings_Q431.pt'))\n",
    "# torch.save(X_test_embeddings, notebooks_path / data_path / Path('X_test_embeddings_Q431.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34425ea1-b1bb-4a59-a2b9-d743f8ecc7a4",
   "metadata": {},
   "source": [
    "## Part 1: fixed pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31cf6be9-1a85-4719-8288-d2cf65d825f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== 1. Define Model ====\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes = 1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.shape[0], -1) # flatten all dimensions but batch\n",
    "        # print(x_flat.shape)\n",
    "        return self.layer(x_flat).squeeze(-1)\n",
    "\n",
    "X_train_embeddings = torch.load(notebooks_path / data_path / Path('X_train_embeddings_Q431.pt')).float()\n",
    "X_val_embeddings = torch.load(notebooks_path / data_path / Path('X_val_embeddings_Q431.pt')).float()\n",
    "X_test_embeddings = torch.load(notebooks_path / data_path / Path('X_test_embeddings_Q431.pt')).float()\n",
    "\n",
    "x = X_train_embeddings[0].unsqueeze(0)\n",
    "\n",
    "lp = LinearProbe(input_dim = x.view(1, -1).shape[-1])\n",
    "lp(x).shape\n",
    "# lpP = LinearProbe(input_dim=, num_segments ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2c9e9739-5b00-4f1e-a6b1-2abc8b433aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7036b18e89f4d7fb2a32c48e011b67d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val AUROC: 0.7358, AUPRC: 0.3557\n",
      "[Epoch 2] Val AUROC: 0.7580, AUPRC: 0.3758\n",
      "[Epoch 3] Val AUROC: 0.7743, AUPRC: 0.4015\n",
      "[Epoch 4] Val AUROC: 0.7791, AUPRC: 0.4086\n",
      "[Epoch 5] Val AUROC: 0.7828, AUPRC: 0.4163\n",
      "[Epoch 6] Val AUROC: 0.7850, AUPRC: 0.4212\n",
      "[Epoch 7] Val AUROC: 0.7868, AUPRC: 0.4279\n",
      "[Epoch 8] Val AUROC: 0.7876, AUPRC: 0.4274\n",
      "[Epoch 9] Val AUROC: 0.7897, AUPRC: 0.4334\n",
      "[Epoch 10] Val AUROC: 0.7910, AUPRC: 0.4349\n",
      "[Epoch 11] Val AUROC: 0.7923, AUPRC: 0.4385\n",
      "[Epoch 12] Val AUROC: 0.7936, AUPRC: 0.4427\n",
      "[Epoch 13] Val AUROC: 0.7943, AUPRC: 0.4436\n",
      "[Epoch 14] Val AUROC: 0.7949, AUPRC: 0.4458\n",
      "[Epoch 15] Val AUROC: 0.7952, AUPRC: 0.4452\n",
      "[Epoch 16] Val AUROC: 0.7958, AUPRC: 0.4475\n",
      "[Epoch 17] Val AUROC: 0.7960, AUPRC: 0.4486\n",
      "[Epoch 18] Val AUROC: 0.7970, AUPRC: 0.4500\n",
      "[Epoch 19] Val AUROC: 0.7973, AUPRC: 0.4502\n",
      "[Epoch 20] Val AUROC: 0.7974, AUPRC: 0.4502\n",
      "✅ Finished training. Evaluating on test set...\n",
      "Test AUROC: 0.7825, AUPRC: 0.4133\n"
     ]
    }
   ],
   "source": [
    "# ==== 2. Training & Evaluation Logic ====\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_logits.append(probs.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_score = torch.cat(all_logits).numpy()\n",
    "    \n",
    "    # print(\"y_true:\", np.unique(y_true))\n",
    "    # print(\"y_score shape:\", y_score.shape, \"dtype:\", y_score.dtype)\n",
    "\n",
    "    return {\n",
    "        'AUROC': roc_auc_score(y_true, y_score),\n",
    "        'AUPRC': average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, model, \n",
    "                epochs=20, batch_size=32, lr=1e-3):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Wrap tensors in datasets\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    # Define model\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            # print(xb.shape)\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            logits = model(xb)\n",
    "            # print(logits.device)\n",
    "            # return\n",
    "            # print(logits.shape)\n",
    "            # print(yb.shape)\n",
    "            # return\n",
    "            loss = loss_fn(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        print(f\"[Epoch {epoch+1}] Val AUROC: {val_metrics['AUROC']:.4f}, AUPRC: {val_metrics['AUPRC']:.4f}\")\n",
    "        # return\n",
    "\n",
    "    print(\"✅ Finished training. Evaluating on test set...\")\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    print(f\"Test AUROC: {test_metrics['AUROC']:.4f}, AUPRC: {test_metrics['AUPRC']:.4f}\")\n",
    "\n",
    "    return model, test_metrics\n",
    "\n",
    "model = LinearProbe(input_dim = x.view(1, -1).shape[-1])\n",
    "# print(x.view(1, -1).shape[-1])\n",
    "model, test_metrics = train_model(X_train_embeddings, y_train, X_val_embeddings, y_val, X_test_embeddings, y_test, model, epochs=20, batch_size=32, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7f984-db85-4cb4-a7db-09b94b0bb25a",
   "metadata": {},
   "source": [
    "## Part 2: learnt pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e4c05f-e1ef-43fb-8d99-aad689d07b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a43f1dd1524086a86aa2223ca13969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f5ff10e4ee4bfca16d400d04e9b641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # first we need to get the stacked embeddings -> these are very memory intensive so we can only have one in memory at a time\n",
    "# X_train_embeddings = get_chronos_embeddings(X_train, num_features, pipeline, return_stack = True)\n",
    "# torch.save(X_train_embeddings, notebooks_path / data_path / Path('X_train_embeddings_Q431_stacked.pt'))\n",
    "# del X_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42431002-641a-4873-b481-c117fb0f1629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f56c6969fa554b67bb057e30e5143fd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X_val_embeddings = get_chronos_embeddings(X_val, num_features, pipeline, return_stack = True)\n",
    "# torch.save(X_val_embeddings, notebooks_path / data_path / Path('X_val_embeddings_Q431_stacked.pt'))\n",
    "# del X_val_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d20e46-2ffd-4be4-99e0-79c932ecf18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985554d5599642d5b9c3d42ee61941fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_test_embeddings = get_chronos_embeddings(X_test, num_features, pipeline, return_stack = True)\n",
    "torch.save(X_test_embeddings, notebooks_path / data_path / Path('X_test_embeddings_Q431_stacked.pt'), _use_new_zipfile_serialization=False)\n",
    "del X_test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e18a9f99-28f8-458a-a386-29ec972777d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbe_LearnablePooling(nn.Module): \n",
    "    def __init__(self, input_dim, num_segments,  n_features, n_classes = 1):\n",
    "        super().__init__()\n",
    "        # pooling weights \n",
    "        self.segment_weights = nn.Parameter(torch.randn(1, num_segments, 1))\n",
    "\n",
    "        # linear probe\n",
    "        self.layer = nn.Linear(input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # weight\n",
    "        # Shape: (1, num_segments, 1) -> (batch_size, num_segments, 1)\n",
    "        weights = F.softmax(self.segment_weights, dim=1)  # softmax over segments\n",
    "        weighted = x * weights  # broadcasting over batch and feature dim\n",
    "        pooled = weighted.sum(dim=1)  # sum over segments\n",
    "        return self.layer(pooled).squeeze(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adbc37-21c6-4faa-85c7-7e6a5ee8dcca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04890b47e8af4db892d9028ff31f80cb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "72eb4d0c894545e2a00e929a106afb10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "7aa54cc3f0554b539e64fff08fe92a02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7f9de4434440409ea8f0f874659bd0aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "83fea2978d3f4df9ab5268bed24739f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "87f7bb1f81c34cf1bf7d7a70c3a3e59b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9c57dfbb1bdb42a7a4a1ca7998ad5bea",
       "style": "IPY_MODEL_04890b47e8af4db892d9028ff31f80cb",
       "value": " 10%"
      }
     },
     "8fae25f8f7614541a60564a532d164bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "985554d5599642d5b9c3d42ee61941fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_87f7bb1f81c34cf1bf7d7a70c3a3e59b",
        "IPY_MODEL_a695b8bfde1344c288d9947c3518c14d",
        "IPY_MODEL_acb497c08be64140a0ffc1ec69a80319"
       ],
       "layout": "IPY_MODEL_7f9de4434440409ea8f0f874659bd0aa"
      }
     },
     "9c57dfbb1bdb42a7a4a1ca7998ad5bea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a695b8bfde1344c288d9947c3518c14d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_83fea2978d3f4df9ab5268bed24739f1",
       "max": 41,
       "style": "IPY_MODEL_8fae25f8f7614541a60564a532d164bb",
       "value": 4
      }
     },
     "acb497c08be64140a0ffc1ec69a80319": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_72eb4d0c894545e2a00e929a106afb10",
       "style": "IPY_MODEL_7aa54cc3f0554b539e64fff08fe92a02",
       "value": " 4/41 [00:06&lt;00:56,  1.54s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
