{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2405308b-b436-407c-8e71-c70884fc43b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, IterableDataset\n",
    "from chronos import BaseChronosPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix,\n",
    "    roc_auc_score, precision_recall_curve,\n",
    "    roc_curve, auc\n",
    ")\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from modules import * \n",
    "\n",
    "# override all pandas display limits\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4aa398f-7502-45e1-b2fd-c25a577c80ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-a-filled.parquet -> set_a_filled\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-b.parquet -> set_b\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-a.parquet -> set_a\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-c-filled.parquet -> set_c_filled\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-c.parquet -> set_c\n",
      "Reading /home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/set-b-filled.parquet -> set_b_filled\n"
     ]
    }
   ],
   "source": [
    "# load parquet files\n",
    "data_path = Path(\"../../data\")\n",
    "notebooks_path = Path(os.getcwd())\n",
    "data_dir = {}\n",
    "\n",
    "##unsafe\n",
    "# for file_path in list((notebooks_path / data_path).glob(\"*.parquet\")):\n",
    "#     print(f\"Reading {file_path}\")\n",
    "#     # retrieve the name of the file without the extension for all OS\n",
    "#     data = pd.read_parquet(file_path)\n",
    "#     # if \"Time\" in df.columns:\n",
    "#     #     df[\"Time\"] = pd.to_datetime(df[\"Time\"])\n",
    "#     data_dir[str(file_path).replace(\"\\\\\", \"/\").split(\"/\")[-1].split(\".\")[0].replace(\"-\", \"_\")] = data\n",
    "\n",
    "for file_path in (notebooks_path / data_path).glob(\"set*.parquet\"):\n",
    "    var_name = file_path.stem.replace(\"-\", \"_\")\n",
    "    globals()[var_name] = pd.read_parquet(file_path)\n",
    "    print(f\"Reading {file_path} -> {var_name}\")\n",
    "\n",
    "\n",
    "ID_vars = [\"PatientID\", \"Time\", \"RecordID\"] # TODO \n",
    "# stationary variables\n",
    "stationary_vars = [\"Age\", \"Gender\", \"Height\"] #, \"ICUType\"]\n",
    "# dynamic variables\n",
    "dynamic_vars = set_a.columns.difference(stationary_vars + ID_vars + ['In-hospital_death']).tolist()\n",
    "\n",
    "feature_cols = dynamic_vars + stationary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5ce5cc-b1d4-43bd-bd0a-93c2124d4426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_parquet_for_lstm(key, scaler=None, fit_scaler=False):\n",
    "    labelname = 'In-hospital_death'\n",
    "    df = globals()[f\"set_{key}_filled\"].copy()\n",
    "\n",
    "    # Sort and fill NaNs\n",
    "    df = df.sort_values([\"RecordID\", \"Time\"])\n",
    "    df[feature_cols] = df[feature_cols].fillna(0)\n",
    "\n",
    "    # raise NotImplementedError(\"Encode cathegories\")\n",
    "\n",
    "    # --- Fit scaler on all feature data if requested ---\n",
    "    if fit_scaler or scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[feature_cols])\n",
    "\n",
    "    # --- Apply scaling ---\n",
    "    df[feature_cols] = scaler.transform(df[feature_cols])\n",
    "\n",
    "    # Group by patient\n",
    "    X = []\n",
    "    y = []\n",
    "    for pid, group in df.groupby(\"RecordID\"):\n",
    "        group = group.sort_values(\"Time\")\n",
    "        X.append(group[feature_cols].values)\n",
    "        y.append(group[labelname].iloc[0])\n",
    "\n",
    "    X_tensor = torch.tensor(np.stack(X)).float()  # (n_patients, seq_len, n_features)\n",
    "    y_tensor = torch.tensor(y).float()            # (n_patients,)\n",
    "\n",
    "    return X_tensor, y_tensor, scaler  # return scaler for reuse on val/test\n",
    "\n",
    "\n",
    "X_train, y_train, fitted_scaler = preprocess_parquet_for_lstm(\"a\", fit_scaler = True)\n",
    "# len(preprocess_parquet_for_lstm(\"b\", scaler = fitted_scaler))\n",
    "X_val, y_val , _    = preprocess_parquet_for_lstm(\"b\", scaler = fitted_scaler)\n",
    "X_test, y_test , _  = preprocess_parquet_for_lstm(\"c\", scaler = fitted_scaler)\n",
    "\n",
    "\n",
    "# extract dimensions\n",
    "num_patients, sequence_length, num_features  = X_train.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ce486f-cb67-4fca-a6fc-8ce42797b71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Load the pretrained Chronos model\n",
    "model_id = \"amazon/chronos-t5-small\"  # Example model; choose the one that fits your needs\n",
    "pipeline = BaseChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",  # use \"amazon/chronos-bolt-small\" for the corresponding Chronos-Bolt model\n",
    "    device_map=device,  # use \"cpu\" for CPU inference\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "# Manually move tokenizer boundaries to match model device\n",
    "pipeline.tokenizer.boundaries = pipeline.tokenizer.boundaries.to(pipeline.model.device)\n",
    "\n",
    "# Fix Chronos tokenizer device mismatch when appending EOS token\n",
    "def patched_append_eos_token(self, token_ids, attention_mask):\n",
    "    device = token_ids.device\n",
    "    batch_size = token_ids.shape[0]\n",
    "    eos_tokens = torch.full((batch_size, 1), fill_value=self.config.eos_token_id, device=device)\n",
    "    eos_mask = torch.full((batch_size, 1), fill_value=True, device=device)\n",
    "    token_ids = torch.concat((token_ids, eos_tokens), dim=1)\n",
    "    attention_mask = torch.concat((attention_mask, eos_mask), dim=1)\n",
    "    return token_ids, attention_mask\n",
    "\n",
    "# Patch the method\n",
    "pipeline.tokenizer._append_eos_token = patched_append_eos_token.__get__(pipeline.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c469730-ddbb-432e-9781-ecbdf082bba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_embeddings = get_chronos_embeddings(X_train, num_features, pipeline, return_stack = False)\n",
    "# X_val_embeddings = get_chronos_embeddings(X_val, num_features, pipeline, return_stack = False)\n",
    "# X_test_embeddings = get_chronos_embeddings(X_test, num_features, pipeline, return_stack = False)\n",
    "# torch.save(X_train_embeddings, notebooks_path / data_path / Path('X_train_embeddings_Q431.pt'))\n",
    "# torch.save(X_val_embeddings, notebooks_path / data_path / Path('X_val_embeddings_Q431.pt'))\n",
    "# torch.save(X_test_embeddings, notebooks_path / data_path / Path('X_test_embeddings_Q431.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34425ea1-b1bb-4a59-a2b9-d743f8ecc7a4",
   "metadata": {},
   "source": [
    "## Part 1: fixed pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cf6be9-1a85-4719-8288-d2cf65d825f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==== 1. Define Model ====\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, input_dim, n_classes = 1):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Linear(input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_flat = x.view(x.shape[0], -1) # flatten all dimensions but batch\n",
    "        # print(x_flat.shape)\n",
    "        return self.layer(x_flat).squeeze(-1)\n",
    "\n",
    "X_train_embeddings = torch.load(notebooks_path / data_path / Path('X_train_embeddings_Q431.pt')).float()\n",
    "X_val_embeddings = torch.load(notebooks_path / data_path / Path('X_val_embeddings_Q431.pt')).float()\n",
    "X_test_embeddings = torch.load(notebooks_path / data_path / Path('X_test_embeddings_Q431.pt')).float()\n",
    "\n",
    "x = X_train_embeddings[0].unsqueeze(0)\n",
    "\n",
    "lp = LinearProbe(input_dim = x.view(1, -1).shape[-1])\n",
    "lp(x).shape\n",
    "# lpP = LinearProbe(input_dim=, num_segments ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c9e9739-5b00-4f1e-a6b1-2abc8b433aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50ad80bc221740fdb2c330ee99abda08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Val AUROC: 0.7349, AUPRC: 0.3478\n",
      "[Epoch 2] Val AUROC: 0.7600, AUPRC: 0.3772\n",
      "[Epoch 3] Val AUROC: 0.7730, AUPRC: 0.3997\n",
      "[Epoch 4] Val AUROC: 0.7783, AUPRC: 0.4073\n",
      "[Epoch 5] Val AUROC: 0.7815, AUPRC: 0.4159\n",
      "[Epoch 6] Val AUROC: 0.7857, AUPRC: 0.4223\n",
      "[Epoch 7] Val AUROC: 0.7876, AUPRC: 0.4274\n",
      "[Epoch 8] Val AUROC: 0.7886, AUPRC: 0.4301\n",
      "[Epoch 9] Val AUROC: 0.7901, AUPRC: 0.4332\n",
      "[Epoch 10] Val AUROC: 0.7915, AUPRC: 0.4373\n",
      "[Epoch 11] Val AUROC: 0.7926, AUPRC: 0.4416\n",
      "[Epoch 12] Val AUROC: 0.7934, AUPRC: 0.4431\n",
      "[Epoch 13] Val AUROC: 0.7948, AUPRC: 0.4453\n",
      "[Epoch 14] Val AUROC: 0.7954, AUPRC: 0.4455\n",
      "[Epoch 15] Val AUROC: 0.7957, AUPRC: 0.4481\n",
      "[Epoch 16] Val AUROC: 0.7965, AUPRC: 0.4477\n",
      "[Epoch 17] Val AUROC: 0.7962, AUPRC: 0.4472\n",
      "[Epoch 18] Val AUROC: 0.7968, AUPRC: 0.4473\n",
      "[Epoch 19] Val AUROC: 0.7978, AUPRC: 0.4516\n",
      "[Epoch 20] Val AUROC: 0.7979, AUPRC: 0.4520\n",
      "✅ Finished training. Evaluating on test set...\n",
      "Test AUROC: 0.7826, AUPRC: 0.4153\n"
     ]
    }
   ],
   "source": [
    "# ==== 2. Training & Evaluation Logic ====\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            all_logits.append(probs.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_score = torch.cat(all_logits).numpy()\n",
    "    \n",
    "    # print(\"y_true:\", np.unique(y_true))\n",
    "    # print(\"y_score shape:\", y_score.shape, \"dtype:\", y_score.dtype)\n",
    "\n",
    "    return {\n",
    "        'AUROC': roc_auc_score(y_true, y_score),\n",
    "        'AUPRC': average_precision_score(y_true, y_score)\n",
    "    }\n",
    "    \n",
    "def train_model(X_train, y_train, X_val, y_val, X_test, y_test, model, \n",
    "                epochs=20, batch_size=32, lr=1e-3):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Wrap tensors in datasets\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    val_ds = TensorDataset(X_val, y_val)\n",
    "    test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "    # Define model\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            # print(xb.shape)\n",
    "            xb, yb = xb.to(device), yb.to(device).float()\n",
    "            logits = model(xb)\n",
    "            # print(logits.device)\n",
    "            # return\n",
    "            # print(logits.shape)\n",
    "            # print(yb.shape)\n",
    "            # return\n",
    "            loss = loss_fn(logits, yb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_metrics = evaluate(model, val_loader, device)\n",
    "        print(f\"[Epoch {epoch+1}] Val AUROC: {val_metrics['AUROC']:.4f}, AUPRC: {val_metrics['AUPRC']:.4f}\")\n",
    "        # return\n",
    "\n",
    "    print(\"✅ Finished training. Evaluating on test set...\")\n",
    "    test_metrics = evaluate(model, test_loader, device)\n",
    "    print(f\"Test AUROC: {test_metrics['AUROC']:.4f}, AUPRC: {test_metrics['AUPRC']:.4f}\")\n",
    "\n",
    "    return model, test_metrics\n",
    "\n",
    "model = LinearProbe(input_dim = x.view(1, -1).shape[-1])\n",
    "# print(x.view(1, -1).shape[-1])\n",
    "model, test_metrics = train_model(X_train_embeddings, y_train, X_val_embeddings, y_val, X_test_embeddings, y_test, model, epochs=20, batch_size=32, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7f984-db85-4cb4-a7db-09b94b0bb25a",
   "metadata": {},
   "source": [
    "## Part 2: learnt pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e4c05f-e1ef-43fb-8d99-aad689d07b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # first we need to get the stacked embeddings -> these are very memory intensive so we can only have one in memory at a time\n",
    "# X_train_embeddings = get_chronos_embeddings(X_train, num_features, pipeline, return_stack = True)\n",
    "# torch.save(X_train_embeddings, notebooks_path / data_path / Path('X_train_embeddings_Q431_stacked.pt'))\n",
    "# del X_train_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42431002-641a-4873-b481-c117fb0f1629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_embeddings = get_chronos_embeddings(X_val, num_features, pipeline, return_stack = True)\n",
    "# torch.save(X_val_embeddings, notebooks_path / data_path / Path('X_val_embeddings_Q431_stacked.pt'))\n",
    "# del X_val_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d20e46-2ffd-4be4-99e0-79c932ecf18f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65659ed532a49c49572de37af88669b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# X_test_embeddings = get_chronos_embeddings(X_test, num_features, pipeline, return_stack = True)\n",
    "# torch.save(X_test_embeddings, notebooks_path / data_path / Path('X_test_embeddings_Q431_stacked.pt'), _use_new_zipfile_serialization=False)\n",
    "# del X_test_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8bb12a3b-8307-48d2-ba98-3d8b76e08472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(y_train, notebooks_path / data_path / \"y_train_Q431.pt\")\n",
    "# torch.save(y_test, notebooks_path /data_path / \"y_test_Q431.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e18a9f99-28f8-458a-a386-29ec972777d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model dimensions   \n",
    "X_embeddings = torch.load(notebooks_path / data_path / Path('X_train_embeddings_Q431_stacked.pt'))\n",
    "patients, num_segments, tokens, features = X_embeddings.shape\n",
    "# extract tensor to test the shape\n",
    "X_tensor = X_embeddings[0].unsqueeze(0)\n",
    "del X_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08ac34ff-2069-4ed1-a042-ab302c268130",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorChunkDataset(IterableDataset):\n",
    "    def __init__(self, file_path, batch_size, label_file=None):\n",
    "        super().__init__()\n",
    "        self.file_path = file_path\n",
    "        self.batch_size = batch_size\n",
    "        self.label_file = label_file\n",
    "\n",
    "    def __iter__(self):\n",
    "        data = torch.load(self.file_path, map_location='cpu')  # Lazy load on each worker\n",
    "        num_batches = data.size(0) // self.batch_size\n",
    "        for i in range(num_batches):\n",
    "            x_batch = data[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            if self.label_file:\n",
    "                labels = torch.load(self.label_file, map_location='cpu')\n",
    "                y_batch = labels[i*self.batch_size:(i+1)*self.batch_size]\n",
    "                yield x_batch, y_batch\n",
    "            else:\n",
    "                yield x_batch\n",
    "\n",
    "def train_model(model, train_loader, device, epochs=10, lr=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCEWithLogitsLoss()  # assuming binary classification\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device).float()\n",
    "\n",
    "            # flatten (sequence, feature_embedding) -> -1\n",
    "            # x_batch = x_batch.view(x_batch.size(0), x_batch.size(1), -1)\n",
    "\n",
    "            logits = model(x_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device).float()\n",
    "        x_batch = x_batch.view(x_batch.size(0), x_batch.size(1), -1)\n",
    "\n",
    "        logits = model(x_batch)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        all_preds.append(probs.cpu())\n",
    "        all_labels.append(y_batch.cpu())\n",
    "\n",
    "    y_true = torch.cat(all_labels).numpy()\n",
    "    y_prob = torch.cat(all_preds).numpy()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_prob)\n",
    "    prc_auc = auc(recall, precision)\n",
    "\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "    print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "    print(f\"PRC AUC: {prc_auc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot PRC Curve\n",
    "    plt.figure()\n",
    "    plt.plot(recall, precision, label=f\"PRC curve (AUC = {prc_auc:.2f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curve\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2adbc37-21c6-4faa-85c7-7e6a5ee8dcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "class LinearProbe_LearnablePooling(nn.Module): \n",
    "    def __init__(self, input_dim, num_segments, n_classes = 1):\n",
    "        super().__init__()\n",
    "        # pooling weights \n",
    "        self.segment_weights = nn.Parameter(torch.randn(1, num_segments, 1, 1))\n",
    "\n",
    "        # linear probe\n",
    "        self.layer = nn.Linear(input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # weight\n",
    "        batch_size = x.shape[0]\n",
    "        # Shape: (1, num_segments, 1) -> (batch_size, num_segments, 1, 1)\n",
    "        weights = F.softmax(self.segment_weights, dim=1)  # softmax over segments\n",
    "        # print(weights.shape, x.shape)\n",
    "        # raise Exception()\n",
    "        weighted = x * weights  # broadcasting over batch and feature dim\n",
    "        # print(weighted.shape)\n",
    "        pooled = weighted.sum(dim=1).view(batch_size, -1)  # sum over segments\n",
    "        # print(pooled.shape)\n",
    "        return self.layer(pooled).squeeze(-1)\n",
    "\n",
    "model = LinearProbe_LearnablePooling(input_dim = tokens * features, num_segments = num_segments)\n",
    "print(model(X_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3314e43f-28d7-4e06-bcc9-0ab275242d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ed8fe08b93407caad8cf2802d32b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 50.7765\n",
      "Epoch 2/10, Loss: 47.8519\n",
      "Epoch 3/10, Loss: 46.6418\n",
      "Epoch 4/10, Loss: 45.7434\n",
      "Epoch 5/10, Loss: 45.0319\n",
      "Epoch 6/10, Loss: 44.4401\n",
      "Epoch 7/10, Loss: 43.9277\n",
      "Epoch 8/10, Loss: 43.4702\n",
      "Epoch 9/10, Loss: 43.0520\n",
      "Epoch 10/10, Loss: 42.6635\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/X_test_embeddings_Q431_stacked.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m train_model(model, train_loader, device)\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# eval \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, device)\u001b[39m\n\u001b[32m     48\u001b[39m all_preds = []\n\u001b[32m     49\u001b[39m all_labels = []\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    763\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m764\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    765\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    766\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:42\u001b[39m, in \u001b[36m_IterableDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     data = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.collate_fn(data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mTensorChunkDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     data = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Lazy load on each worker\u001b[39;00m\n\u001b[32m     10\u001b[39m     num_batches = data.size(\u001b[32m0\u001b[39m) // \u001b[38;5;28mself\u001b[39m.batch_size\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/cluster/courses/ml4h/project1env/lib/python3.12/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/pbaertschi/ICU-TimeSeries-Mortality-Prediction/notebooks/4_Foundation_Models/../../data/X_test_embeddings_Q431_stacked.pt'"
     ]
    }
   ],
   "source": [
    "train_file = notebooks_path / data_path / \"X_train_embeddings_Q431_stacked.pt\"\n",
    "train_labels = notebooks_path / data_path / \"y_train_Q431.pt\"\n",
    "test_file = notebooks_path / data_path / \"X_test_embeddings_Q431_stacked.pt\"\n",
    "test_labels = notebooks_path / data_path / \"y_test_Q431.pt\"\n",
    "\n",
    "# hypers\n",
    "batch_size = 32\n",
    "\n",
    "# streamed train dataset\n",
    "train_dataset = TensorChunkDataset(train_file, batch_size=batch_size, label_file=train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=None)\n",
    "\n",
    "# stream test dataset\n",
    "test_dataset = TensorChunkDataset(test_file, batch_size=batch_size, label_file=test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=None)\n",
    "\n",
    "\n",
    "model = LinearProbe_LearnablePooling(input_dim = tokens * features, num_segments = num_segments)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# train\n",
    "train_model(model, train_loader, device)\n",
    "# eval \n",
    "evaluate_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e34e40c-31c1-448d-9dab-1954c65e7f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f63b2436e554f91a3278030c59b22f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "1661e8c81122476588454ffd92ee049e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_e677485709834877a17d58b956999e93",
       "style": "IPY_MODEL_b47994e7bf194267b2065ba659009791",
       "value": " 20/20 [00:10&lt;00:00,  2.03it/s]"
      }
     },
     "1ffcd96d02744c36a0c59e17f9f9e433": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2101fad8ee714b6fb0a298a58bbe0828": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_bb8351a4aaf342a3aa38919ba9f4239d",
       "style": "IPY_MODEL_f3fb018174e8418e83741fc42f227f8f",
       "value": " 41/41 [00:59&lt;00:00,  1.46s/it]"
      }
     },
     "4d2b76044f86412eb232777f5ac472a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c336468cd9a24eb9a0781636bc9aa03c",
       "style": "IPY_MODEL_735f704f418544269205879dfc47be25",
       "value": "100%"
      }
     },
     "50ad80bc221740fdb2c330ee99abda08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_83115f7c33244bfda6ade43c6435a953",
        "IPY_MODEL_790ec4652abf43f3820c9446936fd3e4",
        "IPY_MODEL_1661e8c81122476588454ffd92ee049e"
       ],
       "layout": "IPY_MODEL_0f63b2436e554f91a3278030c59b22f6"
      }
     },
     "51d7fbb7c3404be382f2e9ae6ab2b731": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5daf9191d8bd4061958cf5fe0ccfd1a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "610a55f7daa24a8984f4b3b5509a4dce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "735f704f418544269205879dfc47be25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "790ec4652abf43f3820c9446936fd3e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_e01cdbf3379e4c0a9942a6dcf38f5202",
       "max": 20,
       "style": "IPY_MODEL_de7e8218634e4130b8829a58bb8e1ed0",
       "value": 20
      }
     },
     "83115f7c33244bfda6ade43c6435a953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_f48fcc3bcdf848c59b55c4c958f2ed83",
       "style": "IPY_MODEL_1ffcd96d02744c36a0c59e17f9f9e433",
       "value": "100%"
      }
     },
     "b47994e7bf194267b2065ba659009791": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bb8351a4aaf342a3aa38919ba9f4239d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c336468cd9a24eb9a0781636bc9aa03c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c466349490c3416f80877cb9c1a41005": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_610a55f7daa24a8984f4b3b5509a4dce",
       "max": 41,
       "style": "IPY_MODEL_51d7fbb7c3404be382f2e9ae6ab2b731",
       "value": 41
      }
     },
     "de7e8218634e4130b8829a58bb8e1ed0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e01cdbf3379e4c0a9942a6dcf38f5202": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e65659ed532a49c49572de37af88669b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4d2b76044f86412eb232777f5ac472a4",
        "IPY_MODEL_c466349490c3416f80877cb9c1a41005",
        "IPY_MODEL_2101fad8ee714b6fb0a298a58bbe0828"
       ],
       "layout": "IPY_MODEL_5daf9191d8bd4061958cf5fe0ccfd1a6"
      }
     },
     "e677485709834877a17d58b956999e93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f3fb018174e8418e83741fc42f227f8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f48fcc3bcdf848c59b55c4c958f2ed83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
